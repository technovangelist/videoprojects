{"text":"[INST]Write a paragraph that encourages users to consider a new AI-powered tool, share their thoughts on it, and mention potential projects they might build with it.[/INST]What do you think? Is this a tool you could use? Do you have any ideas of what you might build with it? I'd love to hear about it in the comments below. Thanks so much for watching. Goodbye. "}
{"text":"[INST]Write a paragraph about getting started with developing applications with Ollama using Python as if you were already familiar with it.[/INST]Hey there, I'm Matt and I want to get you up to speed with developing applications with Ollama using Python. I am assuming you already know what Ollama is and how to work with it. If not, I have a 10 minute intro to Ollama right here which should get you up to speed on all the basics. "}
{"text":"[INST]Write a paragraph explaining how to start building applications with Ollama as a Python developer by describing its main components and how they interact.[/INST]But you want to start building applications that leverage it. Great. Let's start by looking at how to access the API. Ollama has two main components. First there is a client. The client is what runs when you type ollama run llama2. It's the REPL that you work with. The other main component is the service. This is what \"ollama serve\" starts up. Normally you don't do this interactively. Instead it runs as a service in the background. That service publishes the API. The CLI is just another API client that uses the standard public API, just like whatever tool you are going to write. "}
{"text":"[INST]Write a paragraph about reviewing REST API endpoints for working with Ollama as a Python developer[/INST]The service publishes a few different REST API endpoints. You can find these at the GitHub repo under docs and then api.md. Lets start by reviewing these endpoints and I'll point out some things to keep in mind. It's pretty important to understand the underlying API before we can get busy with the python library. "}
{"text":"[INST]Write a paragraph summarizing that as a Python developer working with Ollama API, you can perform various tasks such as generating completions, creating models, and more.[/INST]There a few different things you can do with the api. You can generate a completion using the chat or generate endpoints. This means asking a question to a model and getting an answer back. You can create, delete, copy, and list models. You can show model information, such as the system prompt or template. You can push and pull a model to and from ollama.com. And you can generate embeddings. "}
{"text":"[INST]Write a paragraph explaining how to choose between two endpoints (chat and generate) when working with Ollama as a Python developer, considering whether it's for one-off requests or ongoing conversations.[/INST]Lets look at generating a completion. There are two endpoints here: chat and generate. Both can do exactly the same thing, but which one to use depends on your usecase.  If you are just going to ask a model a question as a one off request and not ever hold a conversation with it, then using the generate endpoint makes the most sense. If you need to go back and forth with the model, and manage memory, or context, then the chat endpoint is more convenient. Again, you can do everything with both. But if its just a one off request, then managing the messages in the chat endpoint is a bit of extra work which is only valuable in conversations. "}
{"text":"[INST]Write a paragraph describing how to use the Ollama API as a Python developer to generate something.[/INST]OK, so generate can be found at http://localhost:11434/api/generate. If you want to use the api on a different host, refer to this video that shows you how to host ollama on a different machine. "}
{"text":"[INST]Write the original paragraph that describes working with Ollama as a Python developer.[/INST]For generate there is only a single required parameter: model. This is the name of the model you want to load. If you call generate with just the model name and the model is already loaded, then the unload timeout will be reset to another 5 minutes with one exception we will talk about later."}
{"text":"[INST]Write the original paragraph as instructions for a Python developer working with Ollama, explaining how to use the prompt parameter.[/INST]The prompt parameter is the question you want to ask the model. This will be inserted into the actual model request based on the template either that is defined in the model or the template you specify in this request. "}
{"text":"[INST]As a Python developer working with Ollama, write a paragraph explaining that when working with multimodal models like Ollama, you can provide an array of base64 encoded images, but the model requires conversion to base64 encoding and may not handle multiple images specially.[/INST]Images is used when you work with a multimodal model, such as llava. You can provide an array of images that are base64 encoded. The model can only deal with a base64 encoded image, so you must perform that conversion yourself. We will see that this is a little different in the Python library. This is an array, but I haven't seen it do anything special with more than one image yet. "}
{"text":"[INST]Write a paragraph explaining how Working with Ollama as a Python developer involves interacting with an API that streams JSON responses, each containing model information and tokens. The responses include a 'done' status and stats for the generation.[/INST]Notice that the response is a 'stream' of json blobs, each specifying the model, created_at, response, and done. So most, if not all, of the endpoints respond as a streaming api. Each JSON blob includes a token\b. It looks like most of them are full words but some of these responses show the parts of words, or tokens. Scroll all the way down and you can see the last blob has \"done\" set to true and includes the context that you can provide to the next generate call. It also shows all of the stats for this generation. You'll notice that it does not provide tokens per second, but you can just divide tokens by seconds to come up with that yourself."}
{"text":"[INST]Write a paragraph explaining how setting 'stream' to False can affect response generation in Working with Ollama as a Python developer.[/INST]If you want the response to be a single value after the generation is complete, you can set stream to false. We can see here that the response shows all the tokens in the response joined together. If you set stream to false, then you will have to wait till all the tokens are generated. Setting stream to false will not speed up the response and in fact it may feel slower since you have to wait longer for that first token. But sometimes its useful to turn streaming off if you are returning JSON. "}
{"text":"[INST]When specifying the 'format json' parameter when working with Ollama as a Python developer, also include the phrase 'respond as json' and provide an example schema in the prompt to ensure consistent output.[/INST]So that takes us to the format parameter. This only allows for a single value and that is JSON. Using format json is more than just specifying the format value. You should also say something like, \"respond as json\" in the prompt, and ideally provide an example of the schema in the prompt as well. Otherwise you run the risk of outputting a different schema each time.  "}
{"text":"[INST]To write the original paragraph, explain that passing an array of numbers in output is used as context for continuing a conversation with Ollama as a Python developer. Describe how this process is somewhat unpredictable and may be better served by using the chat endpoint.[/INST]The big array of numbers in this output is the context. If you want to provide this answer and continue the conversation with the model, pass this context to the next call to the generate endpoint. But it’s a bit of a black box that you can't really control. So if I know I am going to need to hold a conversation, I'll stick with the chat endpoint. "}
{"text":"[INST]As a Python developer working with Ollama, write a paragraph explaining that there is an option to handle the system prompt and template yourself entirely, which was used by some early users of the API who had their own system management.[/INST]Raw is there in case you want to handle the system prompt and template yourself entirely. There were some early users of the API that had a system that managed their own system prompts that needed this.  This also removes the context from the output. "}
{"text":"[INST]Write a paragraph that explains how to keep an Ollama model alive for a specified amount of time as a Python developer.[/INST]Finally there is keep_alive. Keep alive defines how long the model should stay in memory. The default is 5 minutes but you can set it to any time you like, or -1 to keep it in memory forever. If you provide just a number to this, then it will default to seconds, but if you provide it as a string, it has to include an s or m or h to indicate the granularity."}
{"text":"[INST]Write a paragraph that explains the parameters for a chat endpoint with Ollama as a Python developer.[/INST]For the chat endpoint, all the parameters are the same with a few exceptions. The context, system, and prompt parameters in generate are replaced with messages. Messages is an array of message objects that can include the system prompt, the prompt from the user, examples of output, and any memory or context you want to send to the model. Each message has a role which can be system, user, or assistant. \"Content\" is the actual text of the message, and optionally \"images\" is the array of images to send for multimodal models. I'll do some more complex examples later when we get to the library.  "}
{"text":"[INST]Write a paragraph explaining that most Ollama API endpoints are simple to understand from their documentation, but focus on writing instructions for using the Python library as an Ollama developer.[/INST]The rest of the endpoints are a bit simpler and should be understandable using the docs. Let's switch gears to working with the Python library. You can learn more about it at github.com/ollama/ollama-python. "}
{"text":"[INST]Write a paragraph explaining how working with Ollama as a Python developer involves leveraging its ability to simplify the switch to and from streaming mode.[/INST]One of the benefits of the Python library is that it simplifies the switch to and from streaming. The function calls return a single object when not streaming, or an Python Generator when they are streaming. "}
{"text":"[INST]Write a paragraph about getting into code and explaining how to work with Ollama in a Python environment.[/INST]Ok, so lets get into the code. This will be part of the VideoProjects repo under intro dev python dash 2024 04 02. "}
{"text":"[INST]To write a paragraph about working with Ollama as a Python developer, start by importing Ollama and then demonstrate the simplest example of generating text using ollama.generate, setting the model to any model and remembering that this will reset the keep alive parameter for this model.[/INST]this first example is 1.py. I'll start by importing ollama.  The simplest example is just ollama.generate and set model to any model. Remember this is going to reset the keep alive parameter for this model."}
{"text":"[INST]Write a paragraph describing how to work with Ollama in Python as if you're setting a prompt to explain why the sky is blue and running it, including details about streaming responses.[/INST]But let's set the prompt to something in 2.py. why is the sky blue. And we can run it. Notice that we get the non streaming response. In the API, it defaults to streaming. But here we default to not streaming. I think this makes more sense, but changing the rest api means breaking a lot of code in the process, so that probably won't happen until there is a need for a new version of the api. Folks are usually pretty reluctant to do that. "}
{"text":"[INST]Write a paragraph explaining how to add a specific code in 3.py and what happens when you run it, then ask the user to write the original paragraph with the context of working with Ollama as a Python developer.[/INST]So add \"stream\" set to true in 3.py, but we don't get much of a response. so lets iterate thru the parts of the output and write that out. "}
{"text":"[INST]Write a paragraph about working with Ollama as a Python developer where you describe an alternative approach to printing a full chunk of data by instead printing just a token that looks better.[/INST]But that's a bit hard to watch, so rather than printing the full chunk, lets print just the token and that looks better. "}
{"text":"[INST]undefined[/INST]We can add the stats at the end by looking for \"done\" is true. Durations in ollama are measured in nanoseconds so we need to multiply it to be useful. "}
{"text":"[INST]Write a paragraph about making a second generate call with Ollama in Python, using the context from the first call, without relying on any external context.[/INST]In 4.py we are going to make a second generate call that remembers the first, then save the value of context from the last blob and then feed it into the context of the second call. Here I have set the prompt to be vague so that we know it was using the context. We can try again without the 'context' set in order to verify that it does what we think. "}
{"text":"[INST]Write a paragraph explaining how to use Ollama in Python by describing an image from a jpg file in this folder to the model and outputting the description as JSON.[/INST]OK, now lets try describing an image in 5.py. I have a jpg of my hand in this folder. Unlike the REST API which expects a base64 encoded string, the python module expects the image as a bytes object. In fact it won't work with a base64 encoded string. Then just have the model describe the image. And that worked just fine. "}
{"text":"[INST]As a Python developer working with Ollama, write a paragraph that describes the usage and differences of the chat endpoint compared to other API endpoints.[/INST]Let's move on to using the chat endpoint. As we saw in the API section, its pretty much the same thing with the exception of how messages work. "}
{"text":"[INST]Write a paragraph explaining how to use Ollama with Python by defining an array of messages, feeding them to the model, and outputting results in a specific JSON format.[/INST]We can try a simple example in 6.py. I'll just set a system prompt and then the user prompt. So that means defining an array of messages. each message includes a role and the content. Then feed messages to the model and spit out the results. "}
{"text":"[INST]Write a paragraph describing how to work with Ollama as a Python developer and that it involves writing a script (7.py) that uses JSON format, an example schema, and example outputs to provide information about cities.[/INST]For 7.py, we will do something i little more complicated, using format json, an example schema, and some example outputs. See the example there? I specify a user said paris and the model responded with information about Paris. This helps the model understand what you need. So now when we ask about amsterdam we should get something interesting. "}
{"text":"[INST]Write a paragraph about Working with Ollama as a Python developer where you explore using a non-local server URL with Ollama.[/INST]There is one more thing I want to show you. Every call we have made to Ollama has been local thus far. so http://localhost:11434 was the base of the url. But what if you setup a server that is not your local machine. "}
{"text":"[INST]To work with Ollama as a Python developer, set up a Linux box on brev.dev, install and configure Ollama and Tailscale, then update your local code to point to the remote host.[/INST]I set up a linux box on brev.dev and called it mattsremoteollamaapi. I installed ollama and pulled llama2. Then installed tailscale and set the machine name in the tailscale admin to mattsremoteollamaapi. Then set the Ollama_host environment variable to 0.0.0.0 and restarted ollama. Now in the code on my machine, change the ollama import and create a new ollama client pointing to the remote host. And now the code just works. "}
{"text":"[INST]Write this paragraph to explain that you expect your Python code using Ollama to work as expected and provide resources for clarification or discussion.[/INST]I think the rest of the endpoints should just make sense. You will find other examples using the code in this repo which is the code for most of my more recent videos. If anything wasn't clear in this video, let me know in the comments below. It's hard to fix the video, but if it’s a glaring omission i can look at making a new video. And join us in the discord at discord.gg/ollama. "}
{"text":"[INST]Write a paragraph about OpenWebUI desktop app review that discusses its unique features and the challenge of choosing the right tool.[/INST]Hi, I'm Matt. One of the unique aspects of Ollama is its dual nature as both an end-user tool and a developer tool with an API endpoint. This allows developers to program against it. In fact, the GitHub repo homepage lists numerous community integrations, including desktop and web tools - over 30, 40, or even 50 entries! With so many options, it can be overwhelming to choose the right one for your needs."}
{"text":"[INST]Write a paragraph about creating a chart and document to compare desktop app review - openwebui tools, but finding significant issues with many of them.[/INST]To help with this decision-making process, I started creating a chart and document on my blog at technovangelist.com that highlights the differences between these various tools. However, as I began exploring the first few integrations, I discovered that many of them had significant issues. It felt like I just highlighting the bad stuff rather than trying to be helpful."}
{"text":"[INST]Write a paragraph about reviewing a desktop app called openwebui by comparing its features with other similar tools and discussing its strengths and weaknesses from your perspective. This could help users choose the best tool for their needs or inspire them to create a better alternative.[/INST]And so I decided to create a video series that looks at each of these tools. My intention is to provide helpful insights into each app, highlighting what makes them compelling and point out what might be missing from their offerings, from my perspective. Perhaps this will help you find the perfect tool for your needs or even inspire you to build your own application that combines the best features of multiple tools. Let's take a closer look at some of these desktop and web apps and see what they have to offer!"}
{"text":"[INST]Write a paragraph stating your personal opinion about the desktop app review - openwebui, clearly disclosing any potential conflict of interest.[/INST]It's important to remember that while I was a founding member of the Ollama team, I am no longer working for Ollama. So these are simply my opinions and not reflective of anything that anyone in the Ollama team believes."}
{"text":"[INST]Write a paragraph about reviewing the OpenWebUI desktop app in the context of Ollama, mentioning its popularity and a mention in an Ollama Discord server.[/INST]Okay, so let's look at one of these tools. With so many it's hard to know where to start, so I decided to choose the application that I think is among the favorites for most people that are working with Ollama. It's the application that I see most often mentioned in the Ollama Discord, which by the way, you can find at discord.gg/ollama. And it's called OpenWebUI. It used to go by something like Ollama WebUI, but I think the author wants to be less tied to Ollama specifically. "}
{"text":"[INST]Write a paragraph about your experience with the desktop app review of Openwebui, mentioning its visual appeal, customizability, suggestions, and unique approach to prompts.[/INST]There are a lot of things to really like about this tool. It is visually very attractive to look at. It has a few themes, so you can tweak it to be the way you prefer, whether it's light mode or dark mode, and I wish dark mode would die...it hurts my eyes. It gives me good suggestions about some things I could try, although I wish those suggestions would change occasionally. And it also has a very opinionated way of working with prompts, which is interesting. I initially thought this was a little distracting but I think I came around to love it.  Let's take a look."}
{"text":"[INST]Write a paragraph expressing frustration with being asked to login when running a desktop app locally and suggest providing login option for hosting[/INST]So when you first bring up the tool, you are asked to login. I hate it when tools do this. If something is running locally, don't make me sign in. If you want to provide this layer as an option for those who want to host it, great, but make it an option. "}
{"text":"[INST]Write a paragraph describing the features and layout of the desktop app review OpenWebUI in detail.[/INST]In this UI, we have a few sections. Front and center is the chat window. At the bottom we can enter in a message and press enter and then see the results in the main window. Right at the top we can choose a model. This is a pretty critical step so I wish it wasn't as visually hidden. On the left we see a big button for a new chat, then curiously sections for modelfiles and prompts, I'll come back to those, and then documents. Then below that is a list of chats you have had along with a search box for searching those chats. "}
{"text":"[INST]Write a desktop app review for OpenWebUI mentioning that your go-to question is 'why is the sky blue' and that you're using the phi3 model which performs well in many use cases, resulting in an amazing response.[/INST]Let's ask a question, why is the sky blue. This is my go to question. And my model is set to phi3 which is a tiny model that is performing beautifully for so many use cases. Pretty quickly we get a pretty amazing response. "}
{"text":"[INST]Write a paragraph that describes how to find OpenWebUI's desktop app review information on ollama.com.[/INST]But I'm already getting ahead of myself a little bit. Let's go to the webpage and try to figure out how to get this thing installed. We can go to ollama.com and then click on the GitHub link. Now this is the repo readme. Scroll all the way down to the end. Well, almost all the way down to the end. And we get to the top of the community integrations list and look at the web and desktop tools. And here is OpenWebUI at the top."}
{"text":"[INST]Write a paragraph discussing a desktop app review for OpenWebUI that mentions its features and frequent updates.[/INST]This takes us to the Github repo for open webui. Looking at the readme, it looks like an incredible tool. I see local rag integration, intuitive interface, code syntax highlighting, theme customization, markdown and latex support, web browsing capability, and so much more. Looks really really cool. And this list of features goes on and on. Let's scroll up closer to the top, and click on the releases link. And this is interesting because we can see that it's not just one release. There was a release just in the last couple of days. And we can scroll through here and we can see that there are releases happening on a pretty regular basis. And they're not small releases, they're not just documentation fixes. They're significant new features that are being added, which is incredible."}
{"text":"[INST]Write a paragraph about the desktop app review of openwebui discussing its installation process including an option for those who do not like Docker.[/INST]So let's look at the installation process. Near the top is how to install. And right below that is a quick start with Docker. Now not everyone loves Docker. Some people don't like it for whatever reason. They might have misunderstood the licensing kerfuffle that happened a few months or a few years ago, thinking that they need to pay for Docker when most individual users don't need to pay anything. "}
{"text":"[INST]Write a paragraph explaining how to run OpenWebUI desktop app review in a local environment using Docker and specifying port numbers and other parameters.[/INST]But if you are working with Docker, and Ollama is running locally, then you can use this command. -d says run this as a daemon process with the UI running in the background. -p publishs the port 8080 in the container to port 3000 on your host. If you would prefer to access it on a different port, replace 3000 with anything else. the add-host parameter says to use ollama on the local machine. -v creates a volume located at open-webui and mounts that to /app/backend/data in the container. name  sets the name of the container and \"restart always\" means that if it stops for any reason, its coming back up. And finally we are specifying that the container registry and image we should use is on ghcr.io. "}
{"text":"[INST]Write a paragraph describing how to review the desktop app openwebui by creating an account on its sign in page.[/INST]But once you have it running, You should be able to visit localhost and the port, which for me is 3000. And here we have that sign in page. The first time we get here we need to create a new account so click sign up and enter you name, an email and password and click create. Now you are dropped into the ui. "}
{"text":"[INST]Write a paragraph explaining how to set up Openwebui desktop app review by selecting a model and adding a question, related to desktop app review.[/INST]The first thing you probably want to do is set the model to use. Come up here to this dropdown and type in the name of a model you want. I have a lot of models available to me because I have downloaded and created so many. Here I have chosen llama3. and now you can add a question. "}
{"text":"[INST]Write a paragraph reviewing the desktop app settings of openwebui, specifically discussing its theme, language, system prompt, and default parameters, noting concerns about redundancy and inconsistency across models.[/INST]lets go to the settings, which you can find up top here or down in the bottom corner at your name. Under general we can set a theme and language. But then there is system prompt. I'm not sure why anyone would want to set, what I assume to be a global system prompt. Does this get overriden on most models, because the system prompt is set in the model. And there is no way to find out without digging around some docs. And then it looks like we can set some more default parameters which you really want to be different for each model...."}
{"text":"[INST]Write a paragraph about the connections settings in Openwebui desktop app review, including setting an OpenAI API URL and key, updating the OLLAMA URL, adding additional hosts, and noting potential issues with multiple hosts.[/INST]Next is connections. we can set an openai api url as well as an api key. or update the url to use for ollama. And I can also add additional hosts. Not sure what happens when I have multiple hosts because I can't choose the host for a model when i set the model to use. So that’s strange. "}
{"text":"[INST]Write a paragraph reviewing the desktop app OpenWebUI, specifically discussing its model management features and their usability.[/INST]For models we can set the host to use to manage models. Its not clear if this is different than the connections screen, and even though I set a second host, i only see the one default host here. To the right is a button to update all models. Unless you only have a small number of models, do not click this button. It updates models in the most inefficient way possible, pulling every model in your list. It hung on my machine. Then you can pull a new model if you know the name of it, though this is a much more awkward way than the cli. You can also delete models from here. "}
{"text":"[INST]Write a paragraph about desktop app review - openwebui and describe its model upload feature and related functionality.[/INST]It's interesting that they offer a way to upload what they call models as the source gguf files, but those are just the model weights and not the full ollama model, so they are a bit useless until you make the model from that. And then you can do some management of models that you access through litellm. "}
{"text":"[INST]Write a paragraph reviewing the desktop app OpenWebUI's interface, specifically discussing functionality options and default prompt suggestions, with a suggestion to improve it by adding default system prompts.[/INST]In interface we have some functionality options and some default prompt suggestions. I'm not sure what the value of this is. I could imagine some default system prompts would be a lot better served here. "}
{"text":"[INST]Write a paragraph about desktop app review - openwebui discussing limitations in audio settings.[/INST]In audio we have the ability to set a speech to text engine, either using whisper locally or using open ai. but we have no control over which whisper models to use or which language you speak. And then there is text to speech and again there is local or openai. The local one is all but useless. "}
{"text":"[INST]Write a paragraph summarizing that an author wants to support more than just one AI model, making it useful for some users, in the context of reviewing the desktop app OpenWebUI.[/INST]for images we can use this to access automatic1111 which is a bit strange, but the author seems to want to support more than just ollama, so that seems useful to some.  And that's all that is really interesting here. "}
{"text":"[INST]Write a paragraph summarizing an experience with OpenWebUI desktop app review, mentioning that everything feels good but there's a point where things get more interesting.[/INST]so far everything feels pretty good about open web ui. Things work roughly the way you would expect and it generally seems to perform well. But here is where we start getting a bit more interesting. "}
{"text":"[INST]Write a paragraph explaining how to use the OpenWebUI desktop app to review multiple models and see their outputs.[/INST]lets come up to the place we can choose a model. There is the dropdown but there is also this plus sign. So if i click this I can add a second model, and a third, and fourth, and so on. So what happens now? Well, ask a question and you shall see. Why is the sky blue. We wait for it to respond, and we can read the answer. Take a look at the top. I set it to use phi3 and llama3 and so now at the bottom I see it showing 2 of 2. Click the arrow. Its now showing me the output of the other model. So now you have a way of configuring multiple models and its going to ask the same question to each. There is no magic here, its going to take longer to ask more models, but it becomes a bit easier to find the right model for you. "}
{"text":"[INST]Write a paragraph about a desktop app review for openwebui where you mention thumbs up and down buttons, lack of info on data usage, inability to extract data from SQLite db, edit answer feature, and incomplete feedback feature.[/INST]There is also a thumbs up and thumbs down button here. The readme refers to some ability to use this to make something better, but there is no info on where the data is going. I couldn't find any way to get the info out. I looked in the SQLite db the app creates and nothing seems to have been added. You can also edit the answer. Again maybe this is to help come up with a series of good questions and answers to help fine tune later, but the feature doesn't seem to be complete. "}
{"text":"[INST]Write a paragraph about desktop app review - openwebui.[/INST]Ok, lets go up to prompts on the top left corner and click the plus sign to add a new prompt. here you can add a phrase you want to use as the beginning of your prompt. Let's try summarize. enter summarize for title and it will default to using the same thing for command. And then enter the prompt. \"Write a short summary of the following text: \". Click Save and Update. "}
{"text":"[INST]Write a paragraph about the openwebui desktop app review and it should be a general opinion for someone of any age[/INST]Now go to a new chat and type the slash key. press enter to select summarize. Go grab some text from somewhere and paste it in and press enter. And we have a summary. But we had to go through a few extra steps there. So go back to that prompt, and add the word CLIPBOARD in double curly braces. Save and update and lets go back to a chat. The text is still in our clipboard, so type slash summarize and enter. It has automatically taken what's in my clipboard and added it to the end of the prompt. That’s awesome. But maybe you want to summarize it differently depending on the age of the person you are going to share this with. Add 'for someone who is' and then age in square brackets 'years old'. Go to the chat and bring that up again. the text gets pasted in, and age is highlighted, waiting for you to enter an age. You can add as many 'variables' in square brackets as you like. And as many prompts as you like. "}
{"text":"[INST]Write a paragraph reviewing the desktop app 'openwebui', mentioning its prompt system and that users should come up with their own prompts.[/INST]On that prompts page, there is also a link to discover a prompt, but it just takes you to the open webui webpage. You can scroll down to find a list of prompts folks have shared, but it really should link you right to the right place. Even so I didn't find that all that useful. I have found the prompts tend to be super generic and its better to come up with your own. that said, the whole slash command to bring up prompts is amazing and it may be my favorite feature of the app. "}
{"text":"[INST]Write a paragraph summarizing that you can ask questions on a webpage but it only provides information from that page and doesn't follow links, now write a review of the desktop app openwebui.[/INST]let's go back to the chat. Now press the pound key, or you may call it the hash key. type https://technovangelist.com. Now you can ask any question about content on that page. It doesn't seem to follow any links though, its just what's on that page. So I can ask about my patreon, but asking about my employment status with ollama results in a very generic answer. "}
{"text":"[INST]Write a paragraph reviewing how to add documents in the desktop app openwebui.[/INST]back up on the top left is a section called documents. click on it and then the plus sign to add a document. You can add one document or a bunch. Unfortunately you won't get any feedback until the process is done. But once the doc shows up, you can also add tags to the document. Now you can come back to the chat and type the hash key again and you will be prompted to choose a document, or a collection which is the tags associated with your docs. It seems to do a pretty good job here. I wish there was some feedback but maybe that will come soon. "}
{"text":"[INST]Write a paragraph describing an issue with desktop app review - openwebui, where changing document settings has no lasting effect and reverts back to default.[/INST]There are some document settings, but they don't seem to make any impact. I have changed them and the next time I go back to this page, they are changed back to the default. Maybe this is something that needs fixing. "}
{"text":"[INST]Write a paragraph describing your experience and suggestions for improving the desktop app review of Openwebui, specifically regarding the Modelfiles section.[/INST]The last thing we haven't looked as is the Modelfiles section. I have no idea why this exists. they should just make a better experience working with models on the ollama site because it feels like this is just trying to do the same thing but just more awkwardly. I wish this would make it easier to take any model, then apply new parameters and system prompt, give the model  a new name and then save it back to the ollama library. The CLI already does that but some folks prefer a UI so would be a nice addition. Also having another key like backslash or pipe that lets me change the model used would be cool too. "}
{"text":"[INST]Write a review of desktop app openwebui discussing its usability and features in relation to CLI tools.[/INST]I think I have covered everything. So what do I think overall? It's a nice tool but some of the weird aspects are hard to get past. the whole user thing is a pain though I am sure they worked hard on that. I wish you could just disable that entirely. What they are doing with the modelfile section doesn't make any sense, but having a better workflow with models would be great. And that button to update models is way to easy to press so either make it harder to press, or update it to compare the hashes first and then do the pull. The slash command to pull up prompts is pretty genius. I love it. But that's the only differentiating feature from the CLI. Is that enough to use it more regularly? I don't know. I know there are a lot of folks who are scared of the command line and will be ok with it being a 1 to 1 version of the cli tool, but that’s not me. "}
{"text":"[INST]Write a paragraph that is a brief review of the desktop app OpenWebUI, mentioning your initial expectations and experience.[/INST]I thought this would be a short video when i started putting it together, and I didn't expect to like it as much as I did. I hope this will be the experience with the other tools on the community integrations list. "}
{"text":"[INST]Write a paragraph inviting readers to share their opinions about a desktop app review, specifically mentioning Open WebUI and asking if they use it or know of a better tool.[/INST]What do you think? Do you use Open WebUI? Is there anything I missed? Do you think there is a better tool out there? Share that with me in the comments below. I love what folks share in the comments. Be sure to subscribe to my newsletter. You can find it at technovangelist.com/newletter. "}
{"text":"[INST]Write a paragraph explaining how to build a quick RAG app in Python, including the importance of embedding and the challenges of working with PDFs.[/INST]Recently I created a few videos about embedding. Embedding is a key part of the process of setting up a RAG, or Retrieval Augmented Generation system. RAG is good for creating a database where you can ask questions to any documents you may have. Those documents could be markdown or text or web pages or PDFs. PDFs are probably the most common even though they are the absolute worst format you could use. A PDF is not designed to make it easy to get text out. In fact it's often used to make it very hard to get intelligible text out of it. "}
{"text":"[INST]Write a paragraph describing how to build a quick RAG app in Python.[/INST]But anyways lets build a decent RAG system that we can work with. I'll be using Python in this one, but there will be a companion video using Typescript coming soon after. I will skip using PDF's because its such a terrible format. That said, its important and I want to see if there is a good pdf to text workflow beyond what the useless pypdf and pymupdf can do. That will have to be a different video at some point in the future. "}
{"text":"[INST]Write a paragraph describing a basic RAG app in Python, including its main components and how it works with a model and database.[/INST]Lets step back and look at what a basic RAG app includes. The main components are a model that you can ask questions to, and a database that stores all the source documents. But its more than just asking questions to a model. You are providing relevant documentation to the model that will hopefully help answer the question a bit better. You don't want to provide full documents because they tend to confuse the model, but rather just the fragments of those docs that are relevant. "}
{"text":"[INST]Write a paragraph explaining that searching a SQL or document database won't work for this project because you need a specialized database for vector embeddings and similarity search, and describe using ChromaDB.[/INST]Doing a search in a sql or document database won't have good results. You really need a database that supports vector embeddings and some sort of similarity search. So for this video, we are going to use ChromaDB. As far as vector databases go, it probably has the fewest features but that means its super simple to understand, really fast, and really easy to get up and running with. "}
{"text":"[INST]Write a paragraph about creating a quick RAG app in Python by splitting up the document into chunks based on number of sentences, and using sent_tokenize from nltk.tokenize to get a list of english sentences.[/INST]So now we need to split up, or chunk, the document. Some folks like to throw around terms like agentic chunking or semantic chunking as an approach. There seems to have been a youtuber that pushed the idea of that working well, but what I have seen work best is chunking based on number of sentences. It's simple, fast, and works great. In Python, the best option seems to be to use sent_tokenize in nltk.tokenize. You pass it some text and you get back a list of sentences. Now this is a list of english sentences. There is some config you can do to work with other languages, but it doesn't really know what to do if there is yaml or json in your text. "}
{"text":"[INST]Write a paragraph that explains how to embed text into an array of numbers using a suitable model for building a quick RAG app in Python.[/INST]With your chunks in hand, you can now embed. Embedding is a process that generates a mathematical representation of the text, in the form of an array of numbers. While it is technically possible to create embeddings with any model, you really want to use an embedding model to do it fast, efficiently, and to get embeddings that perform well. In fact I tried using a regular model in the app we will build and it failed to come up with anything useful. There are three choices when it comes to embedding models in Ollama, as of early April 2024: nomic-embed-text, mxbai-embed-large which is a model from mixed bread.ai, and all-minilm. In some quick testing, nomic  and mixedbread did the best job, and mixedbread took 50% longer to do the embeddings. "}
{"text":"[INST]Write the original paragraph which is about building a quick RAG app in Python by starting with a working ChromaDB instance and then initiating the chroma client to connect to that database[/INST]So lets start building the app. You can find the code for this project in the github repo, technovangelist/videoprojects. Before going through the code, I want to have a working ChromaDB instance. There are a bunch of ways of doing this but to keep it simple, I will just run this command `chroma run --host localhost --port 8000 --path ./my_chroma_data`. Then in my import python file, I will initiate the chroma client to connect to that database, delete the collection if it exists, and then create a new collection. Normally you wouldn't go thru this step, but I want to start over each time I run it for this example."}
{"text":"[INST]Write a paragraph describing how to build a quick RAG app in Python by pulling articles from a website and asking questions about them.[/INST]I want to pull in articles from a website and be able to ask questions about those articles. So I have a file called sourcedocs.txt that lists each url or file on the filesystem that I want to embed. You can see that it’s a list of articles from the macrumors website. How I actually download the files isn't all that relevant here, but you can see how do it in the code in the repo. The output of my readtext function is just the text of the article. Next I chunk up the text using my chunk text by sentence function that is in the \"matts ollama tools\" module. That uses sent_tokenize to create chunks of x number of sentences. Now for each chunk I can embed. Embedding in Ollama is super easy, and even easier when using the python library. Ollama.embeddings and then specify the model name and the text. For the model name I wanted an easy way to change it to test each model. So I have a config file that sets the embed model name and the main model name. I use the config parser to read that file and then can just yank out those settings. Then I save the embedding value to the embed variable. The last step of the import is to add the embed, the source text, and some metadata to the vector db. Most vector databases also need a unique ID for each item stored, so I create that from the source filename and the index of the chunk in my list of chunks. "}
{"text":"[INST]To write this paragraph, describe your process of building a quick RAG app in Python. Include steps such as initializing model names from config, making chromadb connections, running queries, returning relevant documents, and using them to generate prompts.[/INST]Now my database is populated, so I can perform my search. there is some initialization stuff up front like reading in the model names from the config, and making the connection to the chromadb. Then I take the query from the cli args and create the embedding. Then I just run the query. This is part of the chromadb functionality. I can return the top 5 results, or 10, or any other number. And then join all those together into one string, and then put the original query along with those relevant docs into the prompt that goes to the model. Now I can run ollama generate, passing in the name of my model, the prompt, and that I want to stream the response. "}
{"text":"[INST]Write a paragraph explaining how to build a quick RAG (Recurrent Attentional Generator) app in Python by describing a user's experience with searching for different topics and receiving relevant answers, including searching for 'what happened in Taiwan' and 'what is the vision pro', and outputting results as JSON.[/INST]So let's try it out. I'll set my embed model to be nomic embed text and the main model to be dolphin-mistral. Ill run python3 import.py and import all my text. That takes about 20 seconds. Then python3 search.py what happened in taiwan. And I get a good description of the recent earthquake in Taiwan and how it affects TSMC. Now try searching for \"what is the vision pro\" and we get a good answer about that. Even ask about personal voice and we get an answer there."}
{"text":"[INST]Write a paragraph that tells you to switch the main model to gemma:2b and ask about taiwan, or try another model, and also suggests trying different embedding models.[/INST]Try switching the main model to gemma:2b and then ask the same question about taiwan. Or try a different model. Want to try out the different embedding models? go for it. "}
{"text":"[INST]Write a paragraph explaining how to extend a basic RAG application in Python by adding features such as sorting search results by date or filtering by specific dates and also explain how to use a web facility to get relevant documents and import top 5 results before doing a similarity search[/INST]Hopefully you now see whats involved in creating a basic RAG application. There is a lot more we can do here. Maybe since it’s a news site, all the most recent info is more relevant, so add the date of the article to metadata and then sort the results by date. Or if the question specifies a date, it filters down the search to only documents added on those dates. Or maybe your query does a search using the facility on the web page to come up with the list of relevant docs, import and embed the top 5 results, then do the similarity search and get the answer from the model. There is a lot you could do here going forward. "}
{"text":"[INST]Write a paragraph saying goodbye and inviting someone to join a Discord server at discord.gg/ollama for future collaboration.[/INST]I hope that all makes sense. If you have any questions about this, let me know in the comments below. Or join the discord at discord.gg/ollama. And if you have any ideas for future videos let me know about those below as well. Thanks so much for being here. Goodbye. "}
{"text":"[INST]Write a paragraph describing a frustrating experience with debugging ChromaDB UIs and a similar issue while working with Chroma before understanding its functionality.[/INST]If you are using ChromaDB with RAG or any other usecase, debugging can be a bit...on the frustrating side. Back when we were just starting to build out Ollama I was building a simple rag tool for one of the examples. I remember I was just starting to work with Chroma, before understanding how it worked, I hit an issue and I wasn't sure if the problem was with my query, or with getting the data into Chroma. I had no visibility into what Chroma was doing. And so I used the API to try to pull info out but I wasn't seeing anything. Again, I wasn't sure if the problem was my code or my query or maybe there just wasn't any data in the database. "}
{"text":"[INST]Write a paragraph explaining the need to explore ChromaDB's structure and potential benefits of using SQLite for a specific use case, particularly in relation to chromadb UIs.[/INST]What I needed was a way to see what was inside ChromaDB to see where the problem was. There are a few stages we can go through to find out what is going on. First we can look at the base structure ChromaDB uses. And that’s SQLite. Which is pretty interesting. Using SQLite means we can easily run the system with a minimal number of files. And depending on the use case, accessing data in SQLite can be faster than accessing the same data in the filesystem, depending on how the database was architected. "}
{"text":"[INST]Write a paragraph explaining that you have a lot of data that corresponds to what you had added to the ChromaDB database. Mention that this requires some research and understanding of ChromaDB UIs.[/INST]There are a large number of tools for working with SQLite. SQLPro and Base come to mind as really easy tools to work with SQLite. But to see if your data is there, it requires a bit of spelunking and even better, some knowledge about the workings of sql and the structure of the ChromaDB database. But I can open up one of those tools, point it at the SQLite database used by chroma and start poking around the database. After a few tries I can find the data that corresponds to what I had added to the database. "}
{"text":"[INST]Write a paragraph explaining how to use the REST API in the browser for Chroma UIs by going to http://localhost:8000/docs and clicking on endpoints like 'list collections' and trying them out without entering any data.[/INST]But what if you want to go further. Well, another step you can take is to just use the rest api straight from the browser, but there is only so much you can do there with a few clicks. So if you started Chroma locally using port 8000, then you can probably just go to that url and tack on /docs to the end. And you see the API docs. For most of these endpoints you can click the \"try it out\" button. So i can click on the \"list collections\" endpoint and click \"try it out\". Don't enter anything and click execute and I get a list of collections. "}
{"text":"[INST]Write a paragraph about troubleshooting chromadb UIs with embeddings and specify that you should reference this for writing original content.[/INST]So that’s good, but can I see if the embeddings are there? Well, not super easily. Go down to the query endpoint and specify the id of the collection which I got from the previous endpoint. If you execute it now, you get an error about embedding dimensions. That’s because its looking at the word \"string\" as an array and doesn't know what to do. We need an embedding in there. To deal with this I created this simple script on my Mac that embeds some text and adds it to the clipboard. The reason I did this on a mac is that is what I am using. But hey, we have AI, so I used one of the programming models to translate it to something that would work on windows as well as on linux. Those are in the repo technovangelist/videoprojects. I can't verify that they actually work because I didn't actually try it on those other platforms. Now i can run 'nomicembed what is the vision pro' and paste that into the query and execute. I get a response and can see the most related entries. OK, but that required a bit of work. Is there a tool I can use to get the info more easily?"}
{"text":"[INST]Write a paragraph explaining that there isn't a reliable tool to manage vector databases in general, but one promising tool has issues and may become universal.[/INST]The best tool I found...ok, the only reliable tool I found was something called Chroma DB Admin. And that is one of the problems. There isn't a great tool to work with vector db's in general. I started working with Milvus and there is a tool for that. I am sure there are tools for other databases as well, but not a universal tool that exists. OK, that’s not entirely true. I did stumble on one that makes the promise of becoming a universal tool but for now it has lots of issues and I had it working for a second and then it failed. I'll tell you more about that because it has a lot of promise if they can fix a few things. But other than that, there isn't 'one tool to manage them all'...because there is no standard way of working with vectors. Everyone is doing it a bit different. "}
{"text":"[INST]Write a paragraph explaining how to set up chroma db admin on github at flanker/chromadb-admin and run it with bun.[/INST]ok, so chroma db admin. You can find this on github at flanker/chromadb-admin. Just clone it locally, then run the appropriate tool for you. I like using Bun, so I'll do \"bun install\", and then \"bun dev\". Now I can open up the web interface at the URL shown. "}
{"text":"[INST]Write a paragraph describing how to view documents in a vector database and their components, specifically for chromadb UIs.[/INST]You might have to point it to your database, and then a collection. but pretty quickly you see all of your documents in your vector db. And you see all the components of each doc. So here is an entry and I can see the source text, the embedding, and the metadata. "}
{"text":"[INST]Write a paragraph about a tool that manages all your vector databases, whether they are chroma or pinecone or a few others.[/INST]Now I mentioned there was another tool. If videos only had a lifetime of a few days or weeks, I might not even mention this, but they are around for longer, so by the time you see this, it may have become the most awesome tool ever and this will be an interesting historical record. But for now Vector Admin is interesting but awkward and hopefully soon will resolve some issues and will become incredible. The idea is to have a tool that manages all your vector databases whether they are chroma or pinecone or a few others. "}
{"text":"[INST]Write a paragraph about your experiences with the Chromadb UIs that describe an installation process taking too long, requiring unnecessary setup, and having problems with database management.[/INST]My issues are that the install for now takes ages, then it requires this weird overhead of organizations and workspaces without any real benefit . But you create one of each and then can add your databases. And it supports Chroma, but it doesn't seem to support local databases. It will work with Chroma in Docker though, just not if you run the chroma command locally. ok, So I spun up another database in docker and it worked fine. But then i tried to add the local one again and it overwrote the docker one and now I had nothing. So I tried removing the workspace and recreate it, but that didn't help\b. So I removed the org and then was booted out to create a new org, but to do so I had to create a new user, but I couldn't because I had already run the onboarding. So I was stuck and had to reinstall. "}
{"text":"[INST]Write a paragraph explaining how you would improve the chromadb UI by simplifying features and making it easier to add multiple databases, and mention potential problems with editing documents in the database.[/INST]I would think this could be solved by hiding that whole orgs and workspaces stuff until the benefit of that overhead shows up, and then make it a bit easier to add multiple databases. I was really excited about the opportunity to sync vector databases, so I hope this starts to work better in the future. There are also a lot of features around editing documents in the database but that requires recreating the embeds, and the only option for that is to use openai. If you had used ollama to create the embeds with nomic embed, that isn't going to help much. These are all just some simple polish that is needed, as the rest of the product is pretty great. And I am super excited to see where he goes from here. "}
{"text":"[INST]Write an email expressing concern about org and workspace overhead in chromadb UIs, hoping that the issue will be resolved soon.[/INST]I understand the motivation for the org and workspace overhead. It feels like an enterprise kind of feature and those enterprises, well they are the ones who are likely to be the folks who would pay for such a feature. I just hope it all gets tightened up a bit."}
{"text":"[INST]Write a paragraph discussing tools used with Chroma DB and soliciting feedback on their usefulness.[/INST]So those are the tools I know about to work with Chroma DB. Did you know about any of those? Do you know of any others? Do you think this type of tool is useful when working with Chroma? I would love to hear what you think in the comments below. "}
{"text":"[INST]Write about LobeChat being a client on the Ollama GitHub integration list with many updates and potential to be a great front end for Ollama[/INST]There's a new client for Ollama out on the Ollama GitHub integration list and its called LobeChat...well, it's not new, but it is new for me. And it's getting a lot of updates. Every couple of days it seems to be updated a little bit further, adding new features, adding new capabilities. Looking at the website, it feels like its going to be a great front end for Ollama.  And I thought I'd take a look at it in this video."}
{"text":"[INST]Write a paragraph about having a page on your website with an annotated list of LobeChat integrations and links to videos about them, explaining that it's automatically updated and the official repo's list is unsorted.[/INST]How do I know about these updates? Well, I've got a page on my website. Yeah, I know my website doesn't get a lot of love, but there is this one page that shows an annotated list of all the Ollama integrations. And it has links to my videos about those integrations. It's automatically generated every hour or so while my Mac is running. The list on the official github repo is sorted by, well, there is no real sort to it. It's simply where did the author of the tool want to put it in the list. In fact, at one point, there were a couple of tools that were simply broken and they were at the top of the list, which was super frustrating for everyone."}
{"text":"[INST]Write a paragraph about how a feature in LobeChat makes it hard to decide what content to create next because the list of options provided seems outdated and lacks context.[/INST]But every time I look at that list for inspiration for what video should I build out next, it's hard to tell because I don't know if the application's been updated recently or if it's any good or anything. It's just this randomized list of applications."}
{"text":"[INST]Write a paragraph describing your discovery of LobeChat on an updating list and its position shifting over time.[/INST]So looking at my list I saw this Lobechat and decided to take a look. It's not at the top of the list now, but it was when I started looking at it, because this list updates. "}
{"text":"[INST]Write a paragraph describing a website and its documentation in relation to LobeChat, explaining that the site is busy and hard to navigate[/INST]So let's start with their site. Scroll down and we get an overview of features...and a few different ways to go over those features. It's a very busy webpage which makes it kinda hard to figure out how to get started. And its not much easier when you go to the docs."}
{"text":"[INST]Write a paragraph explaining that a software, specifically LobeChat, has a confusing user experience with unclear instructions for getting started.[/INST]So we can come down to the bottom and click on the quick start link, which brings us to something that is not a quick start. It doesn't show us anything about how to get started. Okay. Now I happen to know that they seem to have an online version, as well as a self-hosted version. And so you have to search around for the link for this self-hosting piece. But even then, it's not really clear how to get started. There is no quick start. Eventually, you find this deploy with Docker and deploy with Docker Compose. But both of them seem to suggest you have to have an Open AI API key. And we're using this specifically with Ollama, so why should I need to have an Open AI API key?"}
{"text":"[INST]Write a paragraph describing a process for running LobeChat locally in preparation for integrating with Ollama, mentioning a Docker command and logging into localhost port 3210.[/INST]It's not until you come down to integrating with Ollama that you see this Docker command for running LobeChat locally when you're gonna want work with Ollama. So I'll run this command, and within a few seconds, I'm able to log in to localhost port 3210. Now it's worth noting that in other places in the documentation, it suggests that you have to set some environment variables to get Ollama to work. But it turns out none of that is accurate."}
{"text":"[INST]Write a paragraph about an app installation process on localhost port 3210 with install functionality that doesn't work properly in LobeChat.[/INST]So when you finally start up LobeChat at localhost port 3210, you're greeted with this LobeChat localhost app functionality that you have to install. And if you click install, nothing happens. And if you refresh the page, you get that same dialog pop-up, which is really bizarre. So you just have to close it each time."}
{"text":"[INST]Write an original paragraph describing how to access and configure settings in LobeChat, including the methods of navigation through the menu.[/INST]Now first let's go into settings to see how to configure this. You can get to settings either by clicking the hamburger icon at the top right corner, or the LobeChat icon in the top left corner. They seem to be the same. "}
{"text":"[INST]Write a paragraph about LobeChat offering customizable options such as light mode, dark mode, font sizes, and primary colors.[/INST]Thankfully, they offer a light mode and dark mode for those that want to choose one versus the other. And you can set some font sizes and primary colors and so forth."}
{"text":"[INST]Write a paragraph explaining how you can set up and use various model-based features in LobeChat.[/INST]Now we can set up some system assistants. So there's a model that you can use for naming the topic for whenever you start a new chat. It'll automatically name that topic. And then there's a translation assistant so you can specify a different model for that as well as a model to generate description, avatar, and tags, which I'm not really sure what that's for. But this is really nice, being able to specify different models for different activities is really a really nice idea."}
{"text":"[INST]Write a paragraph about LobeChat discussing different supported language models and their integration strategies.[/INST]So come down to language model and this is really interesting. Ollama is definitely supported, but so are OpenAI and OpenAI hosted on Azure, or Google Gemini, or Anthropic, or Bedrock, or Grok, or OpenRouter, or together, all sorts of things that are supported here, which is pretty cool. But that makes you wonder if it is a least common denominator kind of integration, or are they really taking advantage of each solution? And we're gonna learn the answer to that pretty quickly."}
{"text":"[INST]Write a paragraph describing the settings in LobeChat for proxy address, client-side fetching, model list, and connectivity check.[/INST]So we can set the interface address, strange that they call it a proxy address, but whatever. Next is client side fetching. This one is totally bizarre. You can turn it on or off but it makes absolutely no sense for Ollama. And then there's a model list. Here we can choose from a list of models installed in ollama. Finally there is a connectivity check which is nice."}
{"text":"[INST]Write a paragraph explaining LobeChat's text-to-speech capabilities, including available options and features.[/INST]Next comes text-to-speech. It's unfortunate, but the only options are either OpenAI, which I assume is Whisper hosted on OpenAI, or browser. There isn't an option for having a local Whisper model, which I have found to be extremely powerful, but that's not an option. Then you can also have it automatically stop speech recognition when you are quiet as well. Then there's some options around which model you want to use for that OpenAI, text-to-speech model and speech-to-text model."}
{"text":"[INST]Write a paragraph explaining how default assistant settings can be overridden in LobeChat.[/INST]Next, we have default assistant. A lot of this stuff is bizarre, but down at chat settings, we can, again, a lot of this doesn't seem to make much sense. But down when we get to model settings, we can set a default model, but we can also, I assume, override model settings. I'm not really sure if this is going to ignore the settings that are stored in the model for temperature, and top P, and presence penalty. If it's just going to ignore anything that's already set in the model or what? It's unclear. It's unclear how to say, just take the defaults from the model itself."}
{"text":"[INST]Write about configuring LobeChat's text-to-speech feature, including options for voices and languages and the status of its plugins.[/INST]Then there's a text-to-speech service, and your only option seem to be OpenAI or something called Edge or Microsoft. I would have thought Edge was Microsoft, but maybe not. Then which voice do you want to use for that text-to-speech, and which language do you want to have? Then there's a list of plugins. I have a few enabled, but none of them seem to actually work."}
{"text":"[INST]Write a paragraph explaining the features at the top of the main LobeChat screen and how they relate to interacting with models.[/INST]Now at the main screen for having a chat with a model, let's take a look at these little icons above the input window. First off, there's a little button to choose which model you want to use. We chose the available models earlier in settings, and this just allows us to choose from one of those models. Then we have an image which is supposed to support image uploads. Here I've chosen a Llava model which does support vision, but we're not able to select a image to upload."}
{"text":"[INST]Write a paragraph about LobeChat discussing uncertainty around temperature overriding and accessing original model settings.[/INST]Next, we have temperature. Again, it's unclear if it's always overriding the temperature set in the model, or is there a way to get the temperature that was set in the model by the model author? That's a function of Ollama, but it's unclear if we have access to that."}
{"text":"[INST]In LobeChat, write a paragraph about setting the length of message history and the consequences for older messages.[/INST]Next, we can set how long our history is, how many messages do we want to keep in our history. It's again unclear what happens to older messages. I would imagine that most tools that allow you to set something like this, would summarize any older messages, or is it just forgetting all those old messages?"}
{"text":"[INST]Write a paragraph about voice input not working in LobeChat settings.[/INST]Next comes voice input. Now, no matter what I had chosen in the settings, I couldn't get this to function. So again, this doesn't seem to work. I don't know why."}
{"text":"[INST]Write a paragraph explaining that LobeChat supports many extensions, some of which are not currently functional in certain versions of the software.[/INST]Next comes extensions, and this seemed to be a big part of what LobeChat supports, and there seems to be a lot of different extensions available. Now, if you had an earlier version of LobeChat, there was a way to get these extensions to be enabled, but they didn't work, and now they just don't work."}
{"text":"[INST]Write a paragraph explaining why you think LobeChat's creators got lazy by not adding support for function calling to their model.[/INST]When you hover over that icon, it claims that this model doesn't support function calling, which is flat out not true. This tells me that the creators of this tool got lazy and didn't want to add support here. Function calling is base functionality supported in Ollama and it works for all models and that’s been there for at least 8 months. "}
{"text":"[INST]Write a paragraph about the capabilities of LobeChat, including its integration with Ollama, system prompts, options, and potential features like Fabric.[/INST]Assistants ARE supported even with Ollama. These are simply system prompts which with Ollama would normally be supplied with the model. There are a lot of options here, but to figure out what they are actually doing you need to activate them,  then click this icon, and then most of them don't seem to be all that interesting. This would be a great place to integrate Fabric and similar tools, but that hasn't been done. "}
{"text":"[INST]Write a paragraph explaining LobeChat's limitations and how they affect its functionality when used with certain online services like Ollama.[/INST]That's really about all there is to LobeChat. It seems that either it has a lot of starts of functionality that hasn't been completed or those features only work with the online services. It is very unfortunate that you can't use any of the plugins because LobeChat hasn't enabled function calling when using Ollama, even though Ollama has great support for it. It's also very unfortunate that you can't use images with any of the models in Ollama, even the vision models."}
{"text":"[INST]Write a paragraph about LobeChat being primarily designed for online functionality and catering to users who require offline solutions due to privacy, security concerns, or work circumstances.[/INST]Remember what I said at the beginning about LobeChat being updated a lot? It is, but very little of that work is happening for offline use. This must be for the folks who don't have the privacy and security concerns that would restrict the use of online solutions, who don't work for companies who impose those concerns, or who are never offline. "}
{"text":"[INST]Write a paragraph stating that something visually appealing is not functional and should be removed from a specific context.[/INST]It feels like Ollama was added as an afterthought to LobeChat, and shouldn't really be supported by the application. It's a great looking UI. It just doesn't have any of the functionality one would expect. Really, if I had any influence anymore on the project I would have it removed from the Ollama GitHub repo since it only reflects badly on the project. "}
{"text":"[INST]Write an original paragraph discussing your experience with LobeChat and expressing doubts about its usefulness.[/INST]What do you think? Have you used LobeChat specifically with Ollama? Is there something you think I'm missing? Because I feel like there's got to be something I'm missing, because it seems there is just not a lot there. There has barely been any mention of it on the Ollama discord so maybe I was just the last to find out its not worth using. "}
{"text":"[INST]Write a paragraph about sharing your favorite tools for LobeChat, mentioning a list of integrations that you keep updated.[/INST]Oh well, its always fun to see what other tools exist and how they function, or don't function. Let me know if there is a client you use and think I should cover. I'll try to continue updating my list of integrations so be sure to keep an eye on it whenever you need to find new tools to play with. You can find it at this url....maybe I should reincarnate my old link shortener and fix that. Another day."}
{"text":"[INST]Write a paragraph about improving your search results while maintaining online privacy, specifically related to Perplexica.[/INST]In this video I want to show you how you can do better searches online that are far more private than what you are probably doing now and much faster to get actual real results."}
{"text":"[INST]Write a paragraph about Perplexica that explains how it provides a list of places to go for an answer instead of giving one itself.[/INST]What happens when you do a search online today? Well, let's open up Google right now and do a search for \"what is a black hole\". We get a whole bunch of results. Sometimes we get an AI answer here as well which is something Google is adding on some searches, but we also always get a list of results. Apart from that occasional AI answer, nothing here is really giving us the answer  to \"what is a black hole\". It's giving us a bunch of places we can go to find the answer. Now we're probably gonna learn a lot in the process but its going to take a while."}
{"text":"[INST]Write a paragraph about perplexica that mentions users giving search engines more information to target ads, and how this will happen across multiple platforms.[/INST]To get to this point we have given Google more information about our interests and they can use that information to let their customers target us with their ads. This is going to happen with all the search engines, Google, Bing, and others. Even Duck Duck Go does this and I thought that was the whole reason most folks were ok with their less then stellar results. "}
{"text":"[INST]Write a paragraph about how AI models can provide accurate or inaccurate answers, which may be relevant to Perplexica.[/INST]So maybe you ask the question to an AI model. Perhaps it is ChatGPT or even Llama3 using Ollama. In the case of a black hole we are probably going to get a great answer, but if your question is more timely, you are sometimes going to get an answer that is made up. These are the hallucinations we have all experienced. "}
{"text":"[INST]Write a paragraph that describes wanting to use an AI model to search online, feed relevant info to it, and receive an answer, using Perplexity.ai as an example[/INST]Ideally you want to search online, then feed the relevant info to a model, and get back an answer. You want something like what you get with Perplexity.ai. Now before you go crazy and say perplexity isn't safe or local or private, I am just using this as an example and we will come back to a potential good solution. Not a perfect solution, but a pretty good solution."}
{"text":"[INST]Write a paragraph about going on an Alaskan cruise that pertains to Perplexica.[/INST]We can ask a question. Now I just got back from an Alaskan cruise on the Westerdam which is part of the Holland America line. It's strange to say that when they were acquired by Carnival a while back and are based in Seattle rather than the Netherlands. But anyway, I want to know what ships are in the Holland America line  right now. Perplexity gives me an answer that starts to answer this. It's not complete, but the materials on the site suggest that I might get a more complete answer if I pay to subscribe to Perplexity and get access to their Pro level. But its good enough for our example. What happened here. "}
{"text":"[INST]Write a paragraph that describes perplexica's features, including analyzing input, generating results, providing search options, a discover tab, and a library for previous searches.[/INST]Well first perplexity tries to analyze what I am asking for and then submits that to an Internet search. Then the results are fed to a model and I see the results of that generation. Nice. We also have some options to focus our search to specific information sources. There is also a discover tab to see what others are searching for and a library to store your previous searches which is great. "}
{"text":"[INST]Write a paragraph about perplexica discussing search engine alternatives like Search NG and how they work[/INST]But you will probably notice that what perplexity is doing isn't all that hard. But there are some problems we need to solve to replicate it. The biggest is probably how to do the search. If we want to avoid the major search engines, are there any other options? Well, there is an interesting project called something like search NG. I am assuming that's how they want you to pronounce S E A R X. They refer to it as a meta search engine and from what I can tell, they are doing the actual search on Google and maybe other places too, but every request looks like its coming from a different client. So there is no way they can track it...for the most part. Now this is best when you host Searx NG yourself somewhere, but you get some of the benefits even running it locally. "}
{"text":"[INST]Write a paragraph about Perplexica that mentions its early days, some features not working, one main feature working well, and requires cloning a repo, editing a config file, and running docker containers. Also mention Ollama and its website.[/INST]And that’s what you get when you install Perplexica. Perplexica is still in its early days. There is a lot that isn't working. But the one main feature is working great. To get it up and running, clone the repo, edit the config file pointing it to Ollama and startup the docker containers using this command. And that’s it.  If you aren't familiar with Ollama, then visit Ollama.com and you can get up and running in seconds. "}
{"text":"[INST]Write a paragraph about Perplexica describing its search window with magnifying glass, library page, gear icon settings, and chat model/embedding model options.[/INST]Now open port 3000 on localhost and here we are. Unfortunately you are stuck with dark mode, so that may be enough for a lot of folks to not use it. When you first launch the site, you are at the search window. On the left side there is a magnifying glass which is the discover page, like on perplexity, but it doesn't work yet. And below that is a little book which is the library page to store previous searches, but that also doesn't work. At the bottom is a gear icon for settings. Click that. You'll want to set the chat model and embedding models to whatever you want. I am using llama3 8b for chat and nomic embed for embedding. "}
{"text":"[INST]Write a paragraph explaining how embedding is used to generate an answer, specifically related to Perplexica, where it analyzes the question to find the best search query and then generates an answer based on similar search results.[/INST]Why do we need embedding? Well its creating a kind of specialized RAG system. This isn't where you upload files, but rather it get's the search results, embeds them and stores the embeddings in the vector db, then embeds your question and finds the most similar results. That is then used to generate an answer. And just like Perplexity, it first analyzes your initial question to figure out the best search query on the web to get the best first results. "}
{"text":"[INST]Write a paragraph that discusses the ships part of Holland America and how it pertains to Perplexity.[/INST]Now we can start asking questions. What ships are part of Holland America. in the results we see a lot of what we saw on perplexity. We have a list of sources, we have the answer generated by a model, and we have other related questions we can ask. "}
{"text":"[INST]Write an original paragraph that describes searching and focusing areas similar to perplexity and mention adding pages and features.[/INST]We can also focus our search to the same areas as we saw on perplexity. And that is pretty much it for the product so far. I look forward to seeing the author add those other two pages as well as enable a lite mode to make it a bit more pleasing to use for the rest of us. "}
{"text":"[INST]Write a paragraph comparing Perplexica to Perplexity in terms of functionality and performance.[/INST]Is this a complete replacement for something like Perplexity? Sort of, but not really. Maybe at some point in the future it will get closer. On the downside, its sometimes slower than Perplexity, but not always. I'd say often the answers are better with Perplexica, mostly because they seem to be more complete in other tests I have done. But maybe if you are paying Perplexity that advantage for local goes away. Of course with Perplexica, everything is local, well, except for the actual search, and everything is a whole lot more private so that's a major advantage for Perplexica. Another advantage for Perplexity is that you can upload a doc and include that in your search results, making the results even more specific to you. That isn't available, at least not yet on Perplexica. "}
{"text":"[INST]Write a paragraph thanking someone for suggesting something and mentioning it's not good for a specific purpose, specifically regarding Perplexica.[/INST]But I think this is pretty cool. I really want to thank \"i have a cute nose\" on YouTube for suggesting I take a look at it. Of course in their comment they asked if its good at rag. I think I answered the question, but if you are looking at a rag solution for your docs, then no, this doesn't do that yet. "}
{"text":"[INST]Write a paragraph asking readers about their experience with Perplexica, other similar tools, and suggesting they subscribe to your newsletter or support you on Patreon.[/INST]What do you think. Have you used Perplexica? Have you used any other tools that perform a similar function and does it better? Or how about any other tools you think I should look at. Let me know in the comments below and I can take a look.  I also have a newsletter that I send out that you can subscribe to at technovangelist.com/newsletter. If you want to support me there is a Patreon at patreon.com/technovangelist. "}
{"text":"[INST]Write a video creator's farewell message thanking viewers for joining them, mentioning enjoyment in creating content, and expressing gratitude.[/INST]Well thanks so much for joining me again on this. I have a lot of fun making these videos and I hope you get a lot out of them. Good bye."}